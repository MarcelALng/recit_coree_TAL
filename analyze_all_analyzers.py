#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Analyse NLP ComplÃ¨te - Comparaison des 3 Analyseurs
Tous les discours de Lee Seung Man (1021 discours)
Hannanum, Kkma, Komoran avec filtrage
"""

import json
import time
from collections import Counter
from konlpy.tag import Hannanum, Kkma, Komoran

# Liste de stop words corÃ©ens
KOREAN_STOPWORDS = {
    # Particules
    'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì€', 'ëŠ”', 'ì—', 'ì—ì„œ', 'ì˜', 'ì™€', 'ê³¼', 'ë¡œ', 'ìœ¼ë¡œ',
    'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜', 'ë³´ë‹¤', 'ì²˜ëŸ¼', 'ê°™ì´',
    # Pronoms
    'ë‚˜', 'ë„ˆ', 'ì €', 'ìš°ë¦¬', 'ê·¸', 'ì´', 'ì €', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°',
    'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ëˆ„êµ¬', 'ë¬´ì—‡', 'ì–´ë””', 'ì–¸ì œ', 'ì–´ë–»ê²Œ',
    # Verbes auxiliaires
    'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤',
    # Adverbes temporels
    'ì§€ê¸ˆ', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°',
    # Conjonctions
    'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ë˜', 'ë˜í•œ', 'ë°',
    # Nombres
    'ì¼', 'ì´', 'ì‚¼', 'ì‚¬', 'ì˜¤', 'ìœ¡', 'ì¹ ', 'íŒ”', 'êµ¬', 'ì‹­',
    # Autres mots fonctionnels
    'ë“±', 'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ë…„', 'ì›”', 'ì¼', 'ì¤‘', 'ê°„', 'ë§', 'ì ', 'ë°”'
}

def analyze_with_analyzer(analyzer_name, analyzer, speeches):
    """Analyse tous les discours avec un analyseur donnÃ©"""
    print(f"\n{'='*80}")
    print(f"ğŸ” ANALYSE AVEC {analyzer_name.upper()}")
    print(f"{'='*80}")
    
    start_time = time.time()
    all_nouns = []
    
    for idx, speech in enumerate(speeches, 1):
        if idx % 100 == 0:
            elapsed = time.time() - start_time
            print(f"  Progression: {idx}/{len(speeches)} discours ({elapsed:.1f}s)")
        
        # Combiner tous les paragraphes
        text = " ".join(speech["paragraphs"])
        
        # Extraire les noms
        nouns = analyzer.nouns(text)
        
        # Filtrer les stop words et mots courts
        nouns_filtered = [word for word in nouns if word not in KOREAN_STOPWORDS and len(word) > 1]
        
        all_nouns.extend(nouns_filtered)
    
    total_time = time.time() - start_time
    
    # Calculer les statistiques
    word_freq = Counter(all_nouns)
    top_50 = word_freq.most_common(50)
    
    print(f"\nâœ… Analyse terminÃ©e en {total_time:.2f} secondes")
    print(f"   Vitesse: {len(speeches)/total_time:.1f} discours/seconde")
    print(f"   Total de noms extraits: {len(all_nouns):,}")
    print(f"   Noms uniques: {len(word_freq):,}")
    
    print(f"\nğŸ† TOP 10 MOTS LES PLUS FRÃ‰QUENTS:")
    for rank, (word, count) in enumerate(top_50[:10], 1):
        print(f"  {rank:2d}. {word:20s} : {count:6,d} fois")
    
    return {
        "analyzer": analyzer_name,
        "execution_time_seconds": round(total_time, 2),
        "speeches_per_second": round(len(speeches)/total_time, 2),
        "total_nouns_extracted": len(all_nouns),
        "unique_nouns": len(word_freq),
        "average_nouns_per_speech": round(len(all_nouns)/len(speeches), 1),
        "top_50_words": [
            {
                "rank": rank,
                "word": word,
                "frequency": count,
                "percentage": round(100 * count / len(all_nouns), 2)
            }
            for rank, (word, count) in enumerate(top_50, 1)
        ]
    }

# Charger les discours
print("="*80)
print("ğŸ“– ANALYSE NLP COMPARATIVE - LEE SEUNG MAN (ì´ìŠ¹ë§Œ)")
print("="*80)
print("\nğŸ“‚ Chargement des discours...")
with open("president_texts_Lee_Seung_Man.json", "r", encoding="utf-8") as f:
    speeches = json.load(f)

total_speeches = len(speeches)
print(f"   âœ“ {total_speeches:,} discours chargÃ©s\n")

# Initialiser les analyseurs
print("ğŸ”§ Initialisation des analyseurs...")
hannanum = Hannanum()
kkma = Kkma()
komoran = Komoran()
print("   âœ“ Tous les analyseurs prÃªts\n")

# Analyser avec chaque analyseur
all_results = []

# HANNANUM
result_hannanum = analyze_with_analyzer("Hannanum", hannanum, speeches)
all_results.append(result_hannanum)

# KKMA
result_kkma = analyze_with_analyzer("Kkma", kkma, speeches)
all_results.append(result_kkma)

# KOMORAN
result_komoran = analyze_with_analyzer("Komoran", komoran, speeches)
all_results.append(result_komoran)

# RÃ‰SUMÃ‰ COMPARATIF
print("\n" + "="*80)
print("ğŸ“Š RÃ‰SUMÃ‰ COMPARATIF - TOUS LES ANALYSEURS")
print("="*80)
print(f"\n{'Analyseur':<15} {'Temps (s)':<12} {'Vitesse':<15} {'Noms extraits':<15} {'Noms uniques':<15}")
print("-" * 80)
for result in all_results:
    print(f"{result['analyzer']:<15} {result['execution_time_seconds']:<12.2f} "
          f"{result['speeches_per_second']:<15.1f} "
          f"{result['total_nouns_extracted']:<15,d} "
          f"{result['unique_nouns']:<15,d}")

# Comparaison des Top 10
print("\n" + "="*80)
print("ğŸ† COMPARAISON DES TOP 10 MOTS")
print("="*80)

for result in all_results:
    print(f"\n{result['analyzer']}:")
    for item in result['top_50_words'][:10]:
        print(f"  {item['rank']:2d}. {item['word']:20s} : {item['frequency']:6,d} fois")

# Sauvegarder les rÃ©sultats
output = {
    "metadata": {
        "president": "Lee Seung Man (ì´ìŠ¹ë§Œ)",
        "total_speeches": total_speeches,
        "stopwords_filtering": True,
        "stopwords_count": len(KOREAN_STOPWORDS),
        "analysis_date": "2025-12-09"
    },
    "results": all_results
}

output_file = "lee_seung_man_all_analyzers.json"
with open(output_file, "w", encoding="utf-8") as f:
    json.dump(output, f, ensure_ascii=False, indent=2)

print("\n" + "="*80)
print("ğŸ’¾ RÃ‰SULTATS SAUVEGARDÃ‰S")
print("="*80)
print(f"  Fichier: {output_file}")
print("\nContenu:")
print("  â€¢ RÃ©sultats des 3 analyseurs (Hannanum, Kkma, Komoran)")
print("  â€¢ Temps d'exÃ©cution pour chaque analyseur")
print("  â€¢ Top 50 mots pour chaque analyseur")
print("  â€¢ Statistiques comparatives")

print("\n" + "="*80)
print("âœ… ANALYSE COMPARATIVE COMPLÃˆTE TERMINÃ‰E")
print("="*80)
