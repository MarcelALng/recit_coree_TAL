#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Analyse NLP ComplÃ¨te - TOUS LES PRÃ‰SIDENTS
3 Analyseurs (Hannanum, Kkma, Komoran) avec monitoring CPU/GPU/Watts
"""

import json
import time
import psutil
import subprocess
from collections import Counter
from konlpy.tag import Hannanum, Kkma, Komoran

# Stop words enrichis
KOREAN_STOPWORDS = {
    'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì€', 'ëŠ”', 'ì—', 'ì—ì„œ', 'ì˜', 'ì™€', 'ê³¼', 'ë¡œ', 'ìœ¼ë¡œ',
    'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜', 'ë³´ë‹¤', 'ì²˜ëŸ¼', 'ê°™ì´',
    'ë‚˜', 'ë„ˆ', 'ì €', 'ìš°ë¦¬', 'ê·¸', 'ì´', 'ì €', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°',
    'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ëˆ„êµ¬', 'ë¬´ì—‡', 'ì–´ë””', 'ì–¸ì œ', 'ì–´ë–»ê²Œ',
    'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤',
    'ì§€ê¸ˆ', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°',
    'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ë˜', 'ë˜í•œ', 'ë°',
    'ì¼', 'ì´', 'ì‚¼', 'ì‚¬', 'ì˜¤', 'ìœ¡', 'ì¹ ', 'íŒ”', 'êµ¬', 'ì‹­',
    'ë“±', 'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ë…„', 'ì›”', 'ì¼', 'ì¤‘', 'ê°„', 'ë§', 'ì ', 'ë°”',
    # MÃ©tadonnÃ©es
    'ë‹´í™”', 'ë°•ì‚¬', 'ëŒ€í†µë ¹ì´ìŠ¹ë§Œ', 'ëŒ€í†µë ¹ì´ìŠ¹ë§Œë°•ì‚¬', 'ëŒ€í†µë ¹ì´ìŠ¹ë§Œë°•ì‚¬ë‹´í™”',
    'ê³µë³´ì²˜', 'ê³µë³´ì‹¤', 'í¸', 'ì§‘', 'í›ˆí™”ë¡', 'ì´ëŒ€í†µë ¹', 'ì¤‘ì•™ë¬¸í™”í˜‘íšŒ'
}

def get_gpu_stats():
    """RÃ©cupÃ¨re les stats GPU via nvidia-smi"""
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,power.draw', 
             '--format=csv,noheader,nounits'],
            capture_output=True, text=True, timeout=2
        )
        if result.returncode == 0:
            gpu_util, mem_used, power = result.stdout.strip().split(',')
            return {
                'gpu_utilization': float(gpu_util),
                'memory_used_mb': float(mem_used),
                'power_watts': float(power)
            }
    except:
        pass
    return {'gpu_utilization': 0, 'memory_used_mb': 0, 'power_watts': 0}

def analyze_president(president_name, file_path, analyzers):
    """Analyse un prÃ©sident avec les 3 analyseurs"""
    print(f"\n{'='*80}")
    print(f"ğŸ“– PRÃ‰SIDENT: {president_name}")
    print(f"{'='*80}")
    
    # Charger les discours
    with open(file_path, 'r', encoding='utf-8') as f:
        speeches = json.load(f)
    
    total_speeches = len(speeches)
    print(f"   âœ“ {total_speeches:,} discours chargÃ©s\n")
    
    # PrÃ©parer les textes
    texts = [" ".join(speech["paragraphs"]) for speech in speeches]
    
    results = []
    
    for analyzer_name, analyzer in analyzers.items():
        print(f"\nğŸ” Analyse avec {analyzer_name}...")
        
        # Monitoring initial
        cpu_samples = []
        gpu_samples = []
        process = psutil.Process()
        
        start_time = time.time()
        all_nouns = []
        
        for idx, text in enumerate(texts, 1):
            if idx % 100 == 0:
                # Ã‰chantillonner CPU/GPU
                cpu_samples.append(process.cpu_percent(interval=0.1))
                gpu_samples.append(get_gpu_stats())
                
                elapsed = time.time() - start_time
                print(f"  {idx}/{total_speeches} discours ({elapsed:.1f}s)")
            
            nouns = analyzer.nouns(text)
            nouns_filtered = [w for w in nouns if w not in KOREAN_STOPWORDS 
                            and len(w) > 1 and not w.isdigit()]
            all_nouns.extend(nouns_filtered)
        
        total_time = time.time() - start_time
        
        # Stats finales
        word_freq = Counter(all_nouns)
        top_50 = word_freq.most_common(50)
        
        # Moyennes CPU/GPU
        avg_cpu = sum(cpu_samples) / len(cpu_samples) if cpu_samples else 0
        avg_gpu_util = sum(s['gpu_utilization'] for s in gpu_samples) / len(gpu_samples) if gpu_samples else 0
        avg_power = sum(s['power_watts'] for s in gpu_samples) / len(gpu_samples) if gpu_samples else 0
        
        print(f"   âœ“ TerminÃ© en {total_time:.2f}s")
        print(f"   CPU moyen: {avg_cpu:.1f}%")
        print(f"   GPU moyen: {avg_gpu_util:.1f}%")
        print(f"   Puissance: {avg_power:.1f}W")
        
        results.append({
            'analyzer': analyzer_name,
            'execution_time_seconds': round(total_time, 2),
            'speeches_per_second': round(total_speeches/total_time, 2),
            'total_nouns': len(all_nouns),
            'unique_nouns': len(word_freq),
            'avg_cpu_percent': round(avg_cpu, 1),
            'avg_gpu_percent': round(avg_gpu_util, 1),
            'avg_power_watts': round(avg_power, 1),
            'top_50_words': [{'rank': i+1, 'word': w, 'frequency': c} 
                           for i, (w, c) in enumerate(top_50)]
        })
    
    return {
        'president': president_name,
        'total_speeches': total_speeches,
        'results': results
    }

# Liste des prÃ©sidents
PRESIDENTS = [
    ('Choi_Kyu_Hah', 'Choi Kyu Hah (ìµœê·œí•˜)'),
    ('Chun_Doo_Hwan', 'Chun Doo Hwan (ì „ë‘í™˜)'),
    ('Kim_Dae_Jung', 'Kim Dae Jung (ê¹€ëŒ€ì¤‘)'),
    ('Kim_Young_Sam', 'Kim Young Sam (ê¹€ì˜ì‚¼)'),
    ('Lee_Myung_Bak', 'Lee Myung Bak (ì´ëª…ë°•)'),
    ('Lee_Seung_Man', 'Lee Seung Man (ì´ìŠ¹ë§Œ)'),
    ('Moon_Jae_In', 'Moon Jae In (ë¬¸ì¬ì¸)'),
    ('Park_Chung_Hee', 'Park Chung Hee (ë°•ì •í¬)'),
    ('Park_Geun_Hye', 'Park Geun Hye (ë°•ê·¼í˜œ)'),
    ('Roh_Moo_Hyun', 'Roh Moo Hyun (ë…¸ë¬´í˜„)'),
    ('Roh_Tae_Woo', 'Roh Tae Woo (ë…¸íƒœìš°)'),
    ('Yun_Bo_Seon', 'Yun Bo Seon (ìœ¤ë³´ì„ )')
]

print("="*80, flush=True)
print("ğŸ‡°ğŸ‡· ANALYSE NLP - TOUS LES PRÃ‰SIDENTS CORÃ‰ENS", flush=True)
print("="*80, flush=True)
print(f"\nNombre de prÃ©sidents: {len(PRESIDENTS)}", flush=True)
print("Analyseurs: Hannanum, Kkma, Komoran", flush=True)
print("Monitoring: CPU, GPU, Watts\n", flush=True)

# Initialiser les analyseurs
print("ğŸ”§ Initialisation des analyseurs...", flush=True)
analyzers = {
    'Hannanum': Hannanum(),
    'Kkma': Kkma(),
    'Komoran': Komoran()
}
print("   âœ“ Tous prÃªts\n", flush=True)

# Analyser chaque prÃ©sident
all_results = []

for file_id, president_name in PRESIDENTS:
    file_path = f"president_texts_{file_id}.json"
    
    try:
        result = analyze_president(president_name, file_path, analyzers)
        all_results.append(result)
        
        # Sauvegarder individuellement
        output_file = f"nlp_analysis_{file_id}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ SauvegardÃ©: {output_file}")
        
    except FileNotFoundError:
        print(f"âš ï¸  Fichier non trouvÃ©: {file_path}")
    except Exception as e:
        print(f"âŒ Erreur: {e}")

# RÃ©sumÃ© global
print("\n" + "="*80)
print("ğŸ“Š RÃ‰SUMÃ‰ GLOBAL")
print("="*80)

for result in all_results:
    print(f"\n{result['president']} ({result['total_speeches']} discours):")
    for r in result['results']:
        print(f"  {r['analyzer']:10s}: {r['execution_time_seconds']:6.1f}s | "
              f"CPU {r['avg_cpu_percent']:4.1f}% | "
              f"GPU {r['avg_gpu_percent']:4.1f}% | "
              f"{r['avg_power_watts']:5.1f}W")

# Sauvegarder rÃ©sumÃ© global
with open('all_presidents_summary.json', 'w', encoding='utf-8') as f:
    json.dump(all_results, f, ensure_ascii=False, indent=2)

print("\n" + "="*80)
print("âœ… ANALYSE COMPLÃˆTE TERMINÃ‰E")
print("="*80)
print(f"\nFichiers crÃ©Ã©s:")
print(f"  â€¢ nlp_analysis_[president].json (12 fichiers)")
print(f"  â€¢ all_presidents_summary.json (rÃ©sumÃ© global)")
