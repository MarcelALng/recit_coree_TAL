#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Analyse NLP Compl√®te avec Komoran + Filtrage
Tous les discours de Lee Seung Man (1021 discours)
"""

import json
import time
from collections import Counter
from konlpy.tag import Komoran

# Liste de stop words cor√©ens
KOREAN_STOPWORDS = {
    # Particules
    'Ïù¥', 'Í∞Ä', 'ÏùÑ', 'Î•º', 'ÏùÄ', 'Îäî', 'Ïóê', 'ÏóêÏÑú', 'Ïùò', 'ÏôÄ', 'Í≥º', 'Î°ú', 'ÏúºÎ°ú',
    'ÎèÑ', 'Îßå', 'Î∂ÄÌÑ∞', 'ÍπåÏßÄ', 'ÏóêÍ≤å', 'ÌïúÌÖå', 'Íªò', 'Î≥¥Îã§', 'Ï≤òÎüº', 'Í∞ôÏù¥',
    # Pronoms
    'ÎÇò', 'ÎÑà', 'Ï†Ä', 'Ïö∞Î¶¨', 'Í∑∏', 'Ïù¥', 'Ï†Ä', 'Ïó¨Í∏∞', 'Í±∞Í∏∞', 'Ï†ÄÍ∏∞',
    'Ïù¥Í≤É', 'Í∑∏Í≤É', 'Ï†ÄÍ≤É', 'ÎàÑÍµ¨', 'Î¨¥Ïóá', 'Ïñ¥Îîî', 'Ïñ∏Ï†ú', 'Ïñ¥ÎñªÍ≤å',
    # Verbes auxiliaires
    'ÌïòÎã§', 'ÎêòÎã§', 'ÏûàÎã§', 'ÏóÜÎã§', 'Ïù¥Îã§', 'ÏïÑÎãàÎã§',
    # Adverbes temporels
    'ÏßÄÍ∏à', 'Ïò§Îäò', 'Ïñ¥Ï†ú', 'ÎÇ¥Ïùº', 'Ïó¨Í∏∞', 'Í±∞Í∏∞', 'Ï†ÄÍ∏∞',
    # Conjonctions
    'Í∑∏Î¶¨Í≥†', 'Í∑∏Îü¨ÎÇò', 'ÌïòÏßÄÎßå', 'Îòê', 'ÎòêÌïú', 'Î∞è',
    # Nombres
    'Ïùº', 'Ïù¥', 'ÏÇº', 'ÏÇ¨', 'Ïò§', 'Ïú°', 'Ïπ†', 'Ìåî', 'Íµ¨', 'Ïã≠',
    # Autres mots fonctionnels
    'Îì±', 'Í≤É', 'Ïàò', 'Îïå', 'ÎÖÑ', 'Ïõî', 'Ïùº', 'Ï§ë', 'Í∞Ñ', 'Îßê', 'Ï†ê', 'Î∞î'
}

print("="*80)
print("üìñ ANALYSE NLP - LEE SEUNG MAN (Ïù¥ÏäπÎßå)")
print("="*80)
print("\nüîß Configuration:")
print("  ‚Ä¢ Analyseur: Komoran")
print("  ‚Ä¢ Filtrage: AVEC stop words")
print("  ‚Ä¢ Fichier source: president_texts_Lee_Seung_Man.json\n")

# Charger les discours
print("üìÇ Chargement des discours...")
with open("president_texts_Lee_Seung_Man.json", "r", encoding="utf-8") as f:
    speeches = json.load(f)

total_speeches = len(speeches)
print(f"   ‚úì {total_speeches:,} discours charg√©s\n")

# Initialiser Komoran
print("üîß Initialisation de Komoran...")
komoran = Komoran()
print("   ‚úì Analyseur pr√™t\n")

# Analyser tous les discours
print("="*80)
print("üîç ANALYSE EN COURS...")
print("="*80)

start_time = time.time()
all_nouns = []
speech_details = []

for idx, speech in enumerate(speeches, 1):
    # Afficher progression tous les 100 discours
    if idx % 100 == 0:
        elapsed = time.time() - start_time
        print(f"  Progression: {idx}/{total_speeches} discours ({elapsed:.1f}s)")
    
    # Combiner tous les paragraphes
    text = " ".join(speech["paragraphs"])
    
    # Extraire les noms
    nouns = komoran.nouns(text)
    
    # Filtrer les stop words et mots courts
    nouns_filtered = [word for word in nouns if word not in KOREAN_STOPWORDS and len(word) > 1]
    
    # Stocker les d√©tails du discours
    speech_details.append({
        "title": speech["title"],
        "nouns_count": len(nouns_filtered),
        "unique_nouns": len(set(nouns_filtered)),
        "text_length": len(text)
    })
    
    all_nouns.extend(nouns_filtered)

total_time = time.time() - start_time

print(f"\n‚úÖ Analyse termin√©e en {total_time:.2f} secondes")
print(f"   Vitesse: {total_speeches/total_time:.1f} discours/seconde\n")

# Calculer les statistiques
word_freq = Counter(all_nouns)
top_50 = word_freq.most_common(50)

print("="*80)
print("üìä STATISTIQUES GLOBALES")
print("="*80)
print(f"  Total de noms extraits    : {len(all_nouns):,}")
print(f"  Noms uniques              : {len(word_freq):,}")
print(f"  Moyenne par discours      : {len(all_nouns)/total_speeches:.1f} noms")
print(f"  Stop words filtr√©s        : {len(KOREAN_STOPWORDS)}")

print("\n" + "="*80)
print("üèÜ TOP 50 MOTS LES PLUS FR√âQUENTS")
print("="*80)
for rank, (word, count) in enumerate(top_50, 1):
    print(f"  {rank:2d}. {word:20s} : {count:6,d} fois")

# Pr√©parer les r√©sultats pour JSON
results = {
    "metadata": {
        "president": "Lee Seung Man (Ïù¥ÏäπÎßå)",
        "analyzer": "Komoran",
        "stopwords_filtering": True,
        "total_speeches": total_speeches,
        "execution_time_seconds": round(total_time, 2),
        "speeches_per_second": round(total_speeches/total_time, 2),
        "analysis_date": "2025-12-09"
    },
    "statistics": {
        "total_nouns_extracted": len(all_nouns),
        "unique_nouns": len(word_freq),
        "average_nouns_per_speech": round(len(all_nouns)/total_speeches, 1),
        "stopwords_count": len(KOREAN_STOPWORDS)
    },
    "top_50_words": [
        {
            "rank": rank,
            "word": word,
            "frequency": count,
            "percentage": round(100 * count / len(all_nouns), 2)
        }
        for rank, (word, count) in enumerate(top_50, 1)
    ],
    "speech_details": speech_details[:10],  # Premiers 10 discours comme √©chantillon
    "stopwords_used": sorted(list(KOREAN_STOPWORDS))
}

# Sauvegarder les r√©sultats
output_file = "lee_seung_man_nlp_analysis.json"
with open(output_file, "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print("\n" + "="*80)
print("üíæ R√âSULTATS SAUVEGARD√âS")
print("="*80)
print(f"  Fichier: {output_file}")
print(f"  Taille: {len(json.dumps(results, ensure_ascii=False))/1024:.1f} KB")
print("\nContenu du fichier:")
print("  ‚Ä¢ M√©tadonn√©es (pr√©sident, analyseur, temps)")
print("  ‚Ä¢ Statistiques globales")
print("  ‚Ä¢ Top 50 mots les plus fr√©quents")
print("  ‚Ä¢ D√©tails des 10 premiers discours (√©chantillon)")
print("  ‚Ä¢ Liste des stop words utilis√©s")

print("\n" + "="*80)
print("‚úÖ ANALYSE COMPL√àTE TERMIN√âE")
print("="*80)
