#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Analyse NLP ComplÃ¨te sur 100 Discours de Lee Seung Man
Compare 3 analyseurs (Hannanum, Kkma, Komoran) avec et sans filtrage stop words
"""

import json
import time
from collections import Counter
from konlpy.tag import Hannanum, Kkma, Komoran

# Liste de stop words corÃ©ens
KOREAN_STOPWORDS = {
    # Particules
    'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì€', 'ëŠ”', 'ì—', 'ì—ì„œ', 'ì˜', 'ì™€', 'ê³¼', 'ë¡œ', 'ìœ¼ë¡œ',
    'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜', 'ë³´ë‹¤', 'ì²˜ëŸ¼', 'ê°™ì´',
    # Pronoms
    'ë‚˜', 'ë„ˆ', 'ì €', 'ìš°ë¦¬', 'ê·¸', 'ì´', 'ì €', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°',
    'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ëˆ„êµ¬', 'ë¬´ì—‡', 'ì–´ë””', 'ì–¸ì œ', 'ì–´ë–»ê²Œ',
    # Verbes auxiliaires
    'í•˜ë‹¤', 'ë˜ë‹¤', 'ìžˆë‹¤', 'ì—†ë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤',
    # Adverbes temporels
    'ì§€ê¸ˆ', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°',
    # Conjonctions
    'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ë˜', 'ë˜í•œ', 'ë°',
    # Nombres
    'ì¼', 'ì´', 'ì‚¼', 'ì‚¬', 'ì˜¤', 'ìœ¡', 'ì¹ ', 'íŒ”', 'êµ¬', 'ì‹­',
    # Autres mots fonctionnels
    'ë“±', 'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ë…„', 'ì›”', 'ì¼', 'ì¤‘', 'ê°„', 'ë§', 'ì '
}

def analyze_with_analyzer(analyzer_name, analyzer, texts, use_stopwords=False):
    """Analyse les textes avec un analyseur donnÃ©"""
    print(f"\n{'='*80}")
    print(f"ðŸ” Analyse avec {analyzer_name} {'(AVEC filtrage)' if use_stopwords else '(SANS filtrage)'}")
    print(f"{'='*80}")
    
    start_time = time.time()
    all_nouns = []
    
    for idx, text in enumerate(texts, 1):
        if idx % 10 == 0:
            print(f"  Progression: {idx}/100 discours...")
        
        # Extraire les noms
        nouns = analyzer.nouns(text)
        
        # Filtrer si nÃ©cessaire
        if use_stopwords:
            nouns = [word for word in nouns if word not in KOREAN_STOPWORDS and len(word) > 1]
        
        all_nouns.extend(nouns)
    
    elapsed_time = time.time() - start_time
    
    # Calculer les frÃ©quences
    word_freq = Counter(all_nouns)
    top_10 = word_freq.most_common(10)
    
    print(f"\nâœ“ Temps d'exÃ©cution: {elapsed_time:.2f} secondes")
    print(f"âœ“ Total de noms extraits: {len(all_nouns):,}")
    print(f"âœ“ Noms uniques: {len(word_freq):,}")
    print(f"\nðŸ“Š TOP 10 MOTS LES PLUS FRÃ‰QUENTS:")
    for rank, (word, count) in enumerate(top_10, 1):
        print(f"  {rank:2d}. {word:20s} : {count:5,d} fois")
    
    return {
        "analyzer": analyzer_name,
        "with_stopwords_filter": use_stopwords,
        "execution_time_seconds": round(elapsed_time, 2),
        "total_nouns": len(all_nouns),
        "unique_nouns": len(word_freq),
        "top_10_words": [(word, count) for word, count in top_10]
    }

# Charger les discours
print("ðŸ“– Chargement des discours de Lee Seung Man...")
with open("president_texts_Lee_Seung_Man.json", "r", encoding="utf-8") as f:
    speeches = json.load(f)

# Prendre les 100 premiers discours
speeches_100 = speeches[:100]
print(f"   âœ“ {len(speeches_100)} discours chargÃ©s")

# Combiner tous les paragraphes de chaque discours
texts = []
for speech in speeches_100:
    combined_text = " ".join(speech["paragraphs"])
    texts.append(combined_text)

print(f"   âœ“ Textes prÃ©parÃ©s pour l'analyse\n")

# Initialiser les analyseurs
print("ðŸ”§ Initialisation des analyseurs...")
hannanum = Hannanum()
kkma = Kkma()
komoran = Komoran()
print("   âœ“ Analyseurs prÃªts\n")

# Stocker tous les rÃ©sultats
all_results = []

# ========== HANNANUM ==========
print("\n" + "="*80)
print("HANNANUM - Analyse complÃ¨te")
print("="*80)

result = analyze_with_analyzer("Hannanum", hannanum, texts, use_stopwords=False)
all_results.append(result)

result = analyze_with_analyzer("Hannanum", hannanum, texts, use_stopwords=True)
all_results.append(result)

# ========== KKMA ==========
print("\n" + "="*80)
print("KKMA - Analyse complÃ¨te")
print("="*80)

result = analyze_with_analyzer("Kkma", kkma, texts, use_stopwords=False)
all_results.append(result)

result = analyze_with_analyzer("Kkma", kkma, texts, use_stopwords=True)
all_results.append(result)

# ========== KOMORAN ==========
print("\n" + "="*80)
print("KOMORAN - Analyse complÃ¨te")
print("="*80)

result = analyze_with_analyzer("Komoran", komoran, texts, use_stopwords=False)
all_results.append(result)

result = analyze_with_analyzer("Komoran", komoran, texts, use_stopwords=True)
all_results.append(result)

# ========== RÃ‰SUMÃ‰ COMPARATIF ==========
print("\n" + "="*80)
print("ðŸ“Š RÃ‰SUMÃ‰ COMPARATIF - TEMPS D'EXÃ‰CUTION")
print("="*80)
print(f"\n{'Analyseur':<15} {'Filtrage':<15} {'Temps (sec)':<15} {'Noms extraits':<15}")
print("-" * 80)
for result in all_results:
    filtrage = "AVEC" if result["with_stopwords_filter"] else "SANS"
    print(f"{result['analyzer']:<15} {filtrage:<15} {result['execution_time_seconds']:<15.2f} {result['total_nouns']:<15,d}")

# Sauvegarder les rÃ©sultats
output = {
    "metadata": {
        "total_speeches_analyzed": len(speeches_100),
        "stopwords_count": len(KOREAN_STOPWORDS),
        "analysis_date": "2025-12-09"
    },
    "results": all_results
}

with open("comprehensive_nlp_results.json", "w", encoding="utf-8") as f:
    json.dump(output, f, ensure_ascii=False, indent=2)

print("\n" + "="*80)
print("âœ… ANALYSE TERMINÃ‰E")
print("="*80)
print("\nðŸ’¾ RÃ©sultats sauvegardÃ©s dans: comprehensive_nlp_results.json")
print("\nCe fichier contient:")
print("  â€¢ Temps d'exÃ©cution pour chaque analyseur")
print("  â€¢ Top 10 mots avec et sans filtrage")
print("  â€¢ Statistiques complÃ¨tes")
