#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
DÃ©monstration du filtrage des stop words en corÃ©en
Comparaison avec et sans stop words sur les discours prÃ©sidentiels
"""

import json
from konlpy.tag import Komoran
from collections import Counter

# Liste de stop words corÃ©ens couramment utilisÃ©s
KOREAN_STOPWORDS = {
    # Particules (ì¡°ì‚¬)
    'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì€', 'ëŠ”', 'ì—', 'ì—ì„œ', 'ì˜', 'ì™€', 'ê³¼', 'ë¡œ', 'ìœ¼ë¡œ',
    'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜', 'ë³´ë‹¤', 'ì²˜ëŸ¼', 'ê°™ì´',
    
    # Pronoms (ëŒ€ëª…ì‚¬)
    'ë‚˜', 'ë„ˆ', 'ì €', 'ìš°ë¦¬', 'ê·¸', 'ì´', 'ì €', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°',
    'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ëˆ„êµ¬', 'ë¬´ì—‡', 'ì–´ë””', 'ì–¸ì œ', 'ì–´ë–»ê²Œ',
    
    # Verbes auxiliaires et terminaisons courantes
    'í•˜ë‹¤', 'ë˜ë‹¤', 'ìžˆë‹¤', 'ì—†ë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤',
    
    # Adverbes temporels/locatifs frÃ©quents
    'ì§€ê¸ˆ', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°',
    
    # Conjonctions
    'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ë˜', 'ë˜í•œ', 'ë°',
    
    # Nombres
    'ì¼', 'ì´', 'ì‚¼', 'ì‚¬', 'ì˜¤', 'ìœ¡', 'ì¹ ', 'íŒ”', 'êµ¬', 'ì‹­',
    
    # Autres mots fonctionnels
    'ë“±', 'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ë…„', 'ì›”', 'ì¼'
}

# Charger les discours
print("ðŸ“– Chargement des discours de Lee Seung Man...")
with open("president_texts_Lee_Seung_Man.json", "r", encoding="utf-8") as f:
    speeches = json.load(f)

# Prendre un Ã©chantillon (premier discours)
sample_speech = speeches[0]
sample_text = " ".join(sample_speech["paragraphs"][:2])  # 2 premiers paragraphes

print(f"   âœ“ Texte Ã©chantillon: {len(sample_text)} caractÃ¨res")
print(f"   Titre: {sample_speech['title']}\n")

# Analyser avec Komoran
print("ðŸ” Analyse morphologique avec Komoran...")
komoran = Komoran()
nouns = komoran.nouns(sample_text)

print(f"   âœ“ {len(nouns)} noms extraits\n")

# Compter les frÃ©quences SANS filtrage
print("=" * 80)
print("ðŸ“Š TOP 20 NOMS LES PLUS FRÃ‰QUENTS (SANS FILTRAGE)")
print("=" * 80)
noun_freq_no_filter = Counter(nouns)
for word, count in noun_freq_no_filter.most_common(20):
    print(f"  {word:15s} : {count:3d} fois")

# Filtrer les stop words
nouns_filtered = [word for word in nouns if word not in KOREAN_STOPWORDS and len(word) > 1]

print("\n" + "=" * 80)
print("ðŸ“Š TOP 20 NOMS LES PLUS FRÃ‰QUENTS (AVEC FILTRAGE)")
print("=" * 80)
noun_freq_filtered = Counter(nouns_filtered)
for word, count in noun_freq_filtered.most_common(20):
    print(f"  {word:15s} : {count:3d} fois")

# Statistiques
print("\n" + "=" * 80)
print("ðŸ“ˆ STATISTIQUES")
print("=" * 80)
print(f"  Noms avant filtrage  : {len(nouns)}")
print(f"  Noms aprÃ¨s filtrage  : {len(nouns_filtered)}")
print(f"  Mots filtrÃ©s         : {len(nouns) - len(nouns_filtered)} ({100*(len(nouns) - len(nouns_filtered))/len(nouns):.1f}%)")
print(f"  Noms uniques (avant) : {len(set(nouns))}")
print(f"  Noms uniques (aprÃ¨s) : {len(set(nouns_filtered))}")

# Sauvegarder les rÃ©sultats
results = {
    "sample_title": sample_speech['title'],
    "sample_length": len(sample_text),
    "total_nouns": len(nouns),
    "filtered_nouns": len(nouns_filtered),
    "stopwords_removed": len(nouns) - len(nouns_filtered),
    "top_20_no_filter": noun_freq_no_filter.most_common(20),
    "top_20_filtered": noun_freq_filtered.most_common(20),
    "stopwords_used": list(KOREAN_STOPWORDS)
}

with open("stopwords_analysis.json", "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print("\nðŸ’¾ RÃ©sultats sauvegardÃ©s dans: stopwords_analysis.json")

# Exemples de stop words trouvÃ©s
stopwords_found = [word for word in nouns if word in KOREAN_STOPWORDS]
print(f"\nðŸš« Exemples de stop words filtrÃ©s: {set(stopwords_found)}")
