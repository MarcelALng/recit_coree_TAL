#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Analyse Textom√©trique Compl√®te - Lee Seung Man
TF-IDF, LDA Topic Modeling, et Visualisations
"""

import json
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # Backend non-interactif
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD
from konlpy.tag import Komoran
import numpy as np
import pandas as pd
from collections import Counter

# Configuration matplotlib pour le cor√©en
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# Stop words
KOREAN_STOPWORDS = {
    'Ïù¥', 'Í∞Ä', 'ÏùÑ', 'Î•º', 'ÏùÄ', 'Îäî', 'Ïóê', 'ÏóêÏÑú', 'Ïùò', 'ÏôÄ', 'Í≥º', 'Î°ú', 'ÏúºÎ°ú',
    'ÎèÑ', 'Îßå', 'Î∂ÄÌÑ∞', 'ÍπåÏßÄ', 'ÏóêÍ≤å', 'ÌïúÌÖå', 'Íªò', 'Î≥¥Îã§', 'Ï≤òÎüº', 'Í∞ôÏù¥',
    'ÎÇò', 'ÎÑà', 'Ï†Ä', 'Ïö∞Î¶¨', 'Í∑∏', 'Ïù¥', 'Ï†Ä', 'Ïó¨Í∏∞', 'Í±∞Í∏∞', 'Ï†ÄÍ∏∞',
    'Ïù¥Í≤É', 'Í∑∏Í≤É', 'Ï†ÄÍ≤É', 'ÎàÑÍµ¨', 'Î¨¥Ïóá', 'Ïñ¥Îîî', 'Ïñ∏Ï†ú', 'Ïñ¥ÎñªÍ≤å',
    'ÌïòÎã§', 'ÎêòÎã§', 'ÏûàÎã§', 'ÏóÜÎã§', 'Ïù¥Îã§', 'ÏïÑÎãàÎã§',
    'ÏßÄÍ∏à', 'Ïò§Îäò', 'Ïñ¥Ï†ú', 'ÎÇ¥Ïùº', 'Ïó¨Í∏∞', 'Í±∞Í∏∞', 'Ï†ÄÍ∏∞',
    'Í∑∏Î¶¨Í≥†', 'Í∑∏Îü¨ÎÇò', 'ÌïòÏßÄÎßå', 'Îòê', 'ÎòêÌïú', 'Î∞è',
    'Ïùº', 'Ïù¥', 'ÏÇº', 'ÏÇ¨', 'Ïò§', 'Ïú°', 'Ïπ†', 'Ìåî', 'Íµ¨', 'Ïã≠',
    'Îì±', 'Í≤É', 'Ïàò', 'Îïå', 'ÎÖÑ', 'Ïõî', 'Ïùº', 'Ï§ë', 'Í∞Ñ', 'Îßê', 'Ï†ê', 'Î∞î',
    'Îã¥Ìôî', 'Î∞ïÏÇ¨', 'ÎåÄÌÜµÎ†πÏù¥ÏäπÎßå', 'Í≥µÎ≥¥Ï≤ò', 'Í≥µÎ≥¥Ïã§', 'Ìé∏', 'Ïßë'
}

print("="*80)
print("üìä ANALYSE TEXTOM√âTRIQUE - LEE SEUNG MAN")
print("="*80)

# Charger les discours
print("\nüìÇ Chargement des discours...")
with open("president_texts_Lee_Seung_Man.json", "r", encoding="utf-8") as f:
    speeches = json.load(f)

print(f"   ‚úì {len(speeches)} discours charg√©s\n")

# Pr√©parer les textes
print("üîß Pr√©paration des textes avec Komoran...")
komoran = Komoran()
documents = []
titles = []

for idx, speech in enumerate(speeches, 1):  # Tous les discours
    if idx % 100 == 0:
        print(f"   Traitement: {idx}/{len(speeches)} discours")
    
    text = " ".join(speech["paragraphs"])
    nouns = komoran.nouns(text)
    nouns_filtered = [w for w in nouns if w not in KOREAN_STOPWORDS 
                     and len(w) > 1 and not w.isdigit()]
    
    documents.append(" ".join(nouns_filtered))
    titles.append(speech["title"][:50])

print(f"   ‚úì {len(documents)} documents pr√©par√©s\n")

# ========== 1. TF-IDF ANALYSIS ==========
print("="*80)
print("üìà 1. ANALYSE TF-IDF")
print("="*80)

tfidf_vectorizer = TfidfVectorizer(max_features=50, min_df=2)
tfidf_matrix = tfidf_vectorizer.fit_transform(documents)
feature_names = tfidf_vectorizer.get_feature_names_out()

# Top mots TF-IDF globaux
tfidf_scores = np.asarray(tfidf_matrix.mean(axis=0)).flatten()
top_indices = tfidf_scores.argsort()[-20:][::-1]

print("\nüèÜ TOP 20 MOTS PAR TF-IDF (Importance globale):")
tfidf_results = []
for idx in top_indices:
    word = feature_names[idx]
    score = tfidf_scores[idx]
    print(f"  {word:15s} : {score:.4f}")
    tfidf_results.append({'word': word, 'tfidf_score': score})

# Visualisation TF-IDF
plt.figure(figsize=(12, 6))
words = [feature_names[i] for i in top_indices[:15]]
scores = [tfidf_scores[i] for i in top_indices[:15]]
plt.barh(range(len(words)), scores)
plt.yticks(range(len(words)), words)
plt.xlabel('TF-IDF Score')
plt.title('Top 15 Mots par TF-IDF - Lee Seung Man')
plt.tight_layout()
plt.savefig('tfidf_analysis.png', dpi=150, bbox_inches='tight')
print("\nüíæ Graphique sauvegard√©: tfidf_analysis.png")

# ========== 2. LDA TOPIC MODELING ==========
print("\n" + "="*80)
print("üéØ 2. LDA TOPIC MODELING")
print("="*80)

n_topics = 5
print(f"\nNombre de topics: {n_topics}")

count_vectorizer = CountVectorizer(max_features=100, min_df=2)
count_matrix = count_vectorizer.fit_transform(documents)
count_features = count_vectorizer.get_feature_names_out()

lda_model = LatentDirichletAllocation(
    n_components=n_topics,
    random_state=42,
    max_iter=20,
    learning_method='online'
)

print("üîÑ Entra√Ænement du mod√®le LDA...")
lda_model.fit(count_matrix)

# Afficher les topics
print("\nüìã TOPICS IDENTIFI√âS:\n")
lda_topics = []
for topic_idx, topic in enumerate(lda_model.components_):
    top_indices = topic.argsort()[-10:][::-1]
    top_words = [count_features[i] for i in top_indices]
    top_scores = [topic[i] for i in top_indices]
    
    print(f"Topic {topic_idx + 1}:")
    print(f"  Mots cl√©s: {', '.join(top_words[:7])}")
    
    lda_topics.append({
        'topic_id': topic_idx + 1,
        'top_words': top_words,
        'scores': [float(s) for s in top_scores]
    })

# Distribution des topics par document
doc_topic_dist = lda_model.transform(count_matrix)

# Visualisation des topics
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for topic_idx in range(n_topics):
    top_indices = lda_model.components_[topic_idx].argsort()[-10:][::-1]
    top_words = [count_features[i] for i in top_indices]
    top_scores = [lda_model.components_[topic_idx][i] for i in top_indices]
    
    axes[topic_idx].barh(range(len(top_words)), top_scores)
    axes[topic_idx].set_yticks(range(len(top_words)))
    axes[topic_idx].set_yticklabels(top_words)
    axes[topic_idx].set_xlabel('Importance')
    axes[topic_idx].set_title(f'Topic {topic_idx + 1}')
    axes[topic_idx].invert_yaxis()

# Supprimer le dernier subplot vide
fig.delaxes(axes[5])

plt.tight_layout()
plt.savefig('lda_topics.png', dpi=150, bbox_inches='tight')
print("\nüíæ Graphique sauvegard√©: lda_topics.png")

# ========== 3. LSA (Latent Semantic Analysis) ==========
print("\n" + "="*80)
print("üî¨ 3. LSA (LATENT SEMANTIC ANALYSIS)")
print("="*80)

n_components = 5
lsa_model = TruncatedSVD(n_components=n_components, random_state=42)

print(f"\nNombre de composantes: {n_components}")
print("üîÑ Entra√Ænement du mod√®le LSA...")

lsa_matrix = lsa_model.fit_transform(tfidf_matrix)

# Afficher les composantes
print("\nüìã COMPOSANTES S√âMANTIQUES:\n")
lsa_components = []
for idx, component in enumerate(lsa_model.components_):
    top_indices = np.abs(component).argsort()[-10:][::-1]
    top_words = [feature_names[i] for i in top_indices]
    top_scores = [component[i] for i in top_indices]
    
    print(f"Composante {idx + 1}:")
    print(f"  Concepts: {', '.join(top_words[:7])}")
    
    lsa_components.append({
        'component_id': idx + 1,
        'top_words': top_words,
        'scores': [float(s) for s in top_scores]
    })

# Variance expliqu√©e
explained_variance = lsa_model.explained_variance_ratio_
print(f"\nüìä Variance expliqu√©e: {explained_variance.sum():.2%}")

# Visualisation LSA
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for comp_idx in range(n_components):
    top_indices = np.abs(lsa_model.components_[comp_idx]).argsort()[-10:][::-1]
    top_words = [feature_names[i] for i in top_indices]
    top_scores = [lsa_model.components_[comp_idx][i] for i in top_indices]
    
    axes[comp_idx].barh(range(len(top_words)), top_scores)
    axes[comp_idx].set_yticks(range(len(top_words)))
    axes[comp_idx].set_yticklabels(top_words)
    axes[comp_idx].set_xlabel('Poids')
    axes[comp_idx].set_title(f'Composante LSA {comp_idx + 1}')
    axes[comp_idx].invert_yaxis()

fig.delaxes(axes[5])
plt.tight_layout()
plt.savefig('lsa_components.png', dpi=150, bbox_inches='tight')
print("\nüíæ Graphique sauvegard√©: lsa_components.png")

# ========== SAUVEGARDER LES R√âSULTATS ==========
results = {
    'president': 'Lee Seung Man (Ïù¥ÏäπÎßå)',
    'total_speeches_analyzed': len(documents),
    'tfidf_analysis': {
        'top_20_words': tfidf_results
    },
    'lda_topics': {
        'n_topics': n_topics,
        'topics': lda_topics
    },
    'lsa_analysis': {
        'n_components': n_components,
        'explained_variance': float(explained_variance.sum()),
        'components': lsa_components
    }
}

with open('textometry_Lee_Seung_Man.json', 'w', encoding='utf-8') as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print("\n" + "="*80)
print("‚úÖ ANALYSE TEXTOM√âTRIQUE TERMIN√âE")
print("="*80)
print("\nFichiers cr√©√©s:")
print("  üìä tfidf_analysis.png - Visualisation TF-IDF")
print("  üéØ lda_topics.png - Topics LDA")
print("  üî¨ lsa_components.png - Composantes LSA")
print("  üìÅ textometry_Lee_Seung_Man.json - R√©sultats complets")
